# 야매장기(Yame Janggi)

## Introduction
This is an implementation of deep-learning AI for Janggi(Korean Chess). All documents, comments and other description are written in Korean.

Janggi : https://en.wikipedia.org/wiki/Janggi

야매장기는 알파고에서 강하게 영감을 받아 딥러닝으로 구현한 장기 AI입니다. 딥러닝은 텐서플로우로 구현되었고, 나머지 게임과 MinMax, 강화 학습 등은 WPF 및 C#으로 되 어 있습니다. 프로젝트 시작한 지는 너무 오래됐는데 제가 관리를 안 해서 점점 시작한 의미가 없어지고 있네요. 앞으로도 별로 건드려볼 생각은 없습니다.. ㅎㅎ


## Features

  알파고의 구현 원리를 참고하되, 학습 방법에서 다소 저의 편의대로 변형이 있을 수 있습니다. 누군가 같이 개발해주면 좋겠지만 아직 특별히 도움을 기대하지 않으므로 저 나름 재미나게 만들 계획입니다.

## Environment / Requirement

 - Visual Studio 2017
 - .Net Framework 4.7
 - Windows 10
 - Python 3.6 64bit
 - TensorFlow

모든 개발은 Visual Studio 2017에서 이루어졌습니다. C#/WPF 코드는 .NET 4.7 환경에서 만들었습니다. TensorFlow를 이용한 딥러닝 알고리즘은 Python 3.6에서 작동됩니다.
본래 학습을 위한 코드는 파이썬 단독으로 이루어지는 것이 마땅하나 레인포스 러닝을 빠르게 돌리기 위해서 결국 C#과 하이브리드 형태로 만들게 되었습니다.

## 프로젝트 구성

 - genMoveSet : 별 거 아닌 보조 프로젝트. 2450가지의 움직임을 생성합니다. 여기서 생성된 결과를 소스 코드에 복사해서 사용했습니다.
 - Janggi : 메인 알고리즘이 구현된 프로젝트입니다.
 - RunnerConsole : 콘솔 환경에서 실행하는 것들이 포함되어있습니다. 간단한 테스트에서부터 지도학습, 강화학습 등이 포함됩니다.
 - RunnerWpf : GUI환경에서 동작합니다. 장기를 직접 두어볼 수 있고, 컴퓨터의 생각 트리를 확인할 수 있습니다.
 - TensorNetworks : 텐서 플로우 프로젝트입니다. TCP/IP로 Janggi 라이브러리와 통신합니다.

## 학습 방법

### 기보 파일

기보 파일은 본 깃허브의 릴리즈에 올려놓았습니다. 야메 장기 사이트 게시판에서 긁어온 겁니다. 야매 장기는 또 어딘가에서 가져온 것이구요..

### 기보 파일 경로 지정

파라미터를 받을 수 있게 해놔야 정상인데 뭐 VS에서는 그냥 코드 고치는게 편해서 ㅋㅋ 모든 경로는 하드코딩되어 있습니다. RunnerConsole프로젝트에서 Process/Train.cs 를 찾아보시고.. 거기서 *.gib 로 검색하시면 genGibo 함수 내부에서 기보 파일이 들어있는 경로를 설정할 수 있습니다.

### 학습 방법 지정

RunnerConsole에서 Process/Train.cs의 Train() 함수를 보시면 세 가지 테스크로 나뉩니다. 첫 번째는 데이터 생성, 두 번째는 증강, 세 번째는 학습인데.. 데이터 생성에서 genGibo() 함수는 기보로부터 학습 파일을 불러오는 것이고.. genReignforce()는 자체 경기를 통해 학습 데이터를 생성합니다.

### 학습 모델 지정

학습 모델은 TensorNetworks라는 파이썬 프로젝트에서 결정합니다. _params.py 파일을 보시면 정책 네트워크와 밸류 네트워크의 모델을 고를 수 있습니다. tensor_network.py와 value_network.py 에는 각각 get_netwrok함수가 있는데, 여기서 문자열을 받아와서 네트워크를 고릅니다.

파이썬은 TCP로 요청을 받아서 동작합니다. 주요 동작으로는 모델 로드(혹은 이닛), training, evaluation, save 이렇게 네 가지 동작이 제공되며 이에 대한 자세한 구현은 tensor_networks.py에 들어있습니다. tcp_comm_server.py에서 명령을 받아와서 함수를 실행합니다. 주고 받는 데이터에 대한 프로토콜 포맷도 여기 파일에 기술되어 있습니다.

### 학습 실행

RunnerConsole과 TensorNetworks를 동시에 실행하셔야 합니다. 솔루션 익스플로러에서 솔루션에 대고 오른쪽 클릭 -> 속성 -> Common -> Start up ... 등등 가시면 설정할 수 있습니다.

모든 프로세스 진행은 C#이 담당하고 파이썬은 단지 네트워크 모듈로서만 작동합니다. C#에 타이머를 걸어놓아서 10분에 한 번씩 저장 명령을 날립니다.

텐서보드는 log 폴더를 참고하세요.

### 학습 결과 활용

결과를 보시려면 RunnerWpf와 TensorNetworks 를 동시에 실행합니다. RnnnerWpf의 MainWikndows.xaml.cs 에서 'net Mcts'로 검색해보세요. Mcts는 각종 변형이 있을 수 있는데 이와 관련된 함수를 인자로 받아옵니다. realYame는 가장 보통의 Mcts 구현이고.... onlyPolicy는 정책 네트워크만 사용한 구현입니다.

[[/images/20170923_01.jpg]]

오른쪽에서 Mcts를 트리구조로 탐색할 수 있고, 각 노드의 장기판을 볼 수도 있습니다. 요거 하나 그나마 쓸만한 거 같네요 ㅋㅋ

## 네트워크 간단 소개

resnet이라고 구현해 놓은 것은 SE 적용된 버전을 올려놨습니다. 장기 학습에는 오히려 단순흔 VGG스타일이 좋을 것 같기도 하네요.

네트워크의 입력은 현 보드의 상황이고, 출력은 이길 확률 혹은 어디를 둬야 할지 입니다. 학습하는 경우에는 정답을 같이 알려줘야겠죠. 보드 입력은 Janggi프로젝트에서 Board.cs 의 GetBytes를 보면 확인할 수 있는데요, 중요한 것은 항상 아래쪽이 둘 차례라는 것입니다.

어디를 둘 지(Move)는 출발점과 도착점으로 나뉘는데, 각각에 대해 따로 Softmax를 적용합니다. 그 다음 현재 보드 상황에서 가능한 움직임을 추려내는데 - 그냥 코드로 계산 - 보통 30여가지 나옵니다. 이 모든 점을 조사한 뒤 출발점 확률 * 도착점 확률 을 계산하면 최종 해당 움직임의 확률이 나오는 것이죠.

단순히 생각하면 출발점(90) * 도착점(90)가지의 움직임이 가능할 것 같지만 실제로는 그렇지 않습니다. 예를 들어 대각선으로 이동하는 돌은 장기에서 없기 때문에 그런 움직임은 고려할 필요가 없죠. 그래서 예전에는 모든 가능한 움직임을 미리 뽑아내고 어쩌구 했었는데.. 그냥 복잡하기만 하고 지금 방식이 좋은 것 같네요.

가치 네트워크는 엄청 간단합니다. 결과가 1이면 아래쪽이 이기는 것이고 0이면 위쪽이 이기는 것이죠.
